\documentclass[leqno]{article}
\usepackage{verbatim}
\usepackage{array} \usepackage{listings}
\usepackage{fancyvrb}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{eso-pic}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{graphicx}

% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}

% geometry
\usepackage{geometry}
\geometry{a4paper, margin=0.5in}

% paragraph length
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{proposition}{Proposition}
\newtheorem*{definition}{Definition}
\newtheorem*{observation}{Observation}

\newcommand{\incfig}[1]{%
\center
\def\svgwidth{0.9\columnwidth}
\import{./figures/}{#1.pdf_tex}
}
\newcommand{\incimg}[1]{%
\center
\includegraphics[width=0.9\columnwidth]{images/#1}
}
\pdfsuppresswarningpagegroup=1

\title{Formulario Estadística}
\author{Abel Doñate Muñoz}
\date{}

\begin{document}
\maketitle
\tableofcontents
\newpage

\textbf{Tabla de distribuciones discretas}
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
	\hline
	$Modelo$ &  $p(X=k)$ &  $E[X]$ &  $Var[X]$ &  $G_X(z)$ \\
	\hline
	\makecell{\textbf{Bernoulli}\\ $\sim Be(p)$ }  & $\begin{cases}  p(X=1)=p \\  p(X=0) = 1-p \end{cases}$ & $p$ &  $p(1-p)$ &  $(1-p) + pz$ \\
	\hline
	\makecell{\textbf{Binomial}\\ $\sim Bin(N, p)$} & $\displaystyle\binom{N}{k}p^i(1-p)^{N-k}$ & $Np$ &  $Np(1-p)$ &  $((1-p)+pz)^N$ \\
	\hline
	\makecell{\textbf{Uniforme} \\ $\sim U(1, N)$} &  $\displaystyle\frac{1}{N}$ & $\displaystyle\frac{N+1}{2}$ & $\displaystyle\frac{N^2-1}{12}$ & $\displaystyle\frac{1}{N} \frac{z(z^N-1)}{z-1}$ \\
	\hline
	\makecell{\textbf{Poisson} \\ $\sim Po(\lambda)$} & $\displaystyle\frac{\lambda^k}{k!}e^{-\lambda}$ & $\lambda$ & $\lambda$ & $\displaystyle e^{\lambda(z-1)}$ \\
	\hline
	\makecell{\textbf{Geométrica} \\ $\sim Geom(p)$} & $\displaystyle p(1-p)^{k-1}$ &  $\displaystyle\frac{1}{p}$ &  $\displaystyle\frac{1-p}{p^2}$ &  $\displaystyle \frac{pz}{1-(1-p)z}$ \\
	\hline
	\makecell{\textbf{Binomial negativa} \\ $\sim BinN(r, p)$} & $\begin{cases} 0 & \text{ si } k<r \\ \binom{k-1}{r-1}p^r(1-p)^{k-r} & \text{ si } k\ge r \end{cases}$ & $\displaystyle\frac{r}{p}$ & $\displaystyle r \frac{1-p}{p^2}$ & $\displaystyle \left( \frac{pz}{1-(1-p)z} \right)^r $ \\
	\hline
  \end{tabular}
\end{center}


\textbf{Tabla de distribuciones continuas}
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
	\hline
	$Modelo$ &  $f_X(x)$ &  $E[X]$ &  $Var[X]$ &  $G_X(z)$ \\
	\hline
	\makecell{\textbf{Uniforme}\\ $\sim U(a,b)$ }  & $\displaystyle\frac{1}{b-a}\mathbb{I}_{[a,b]}$ & $\displaystyle \frac{b+a}{2}$ &  $\displaystyle \frac{(b-a)^2}{12}$ &  $1 $ en $\displaystyle t=0, \frac{e^{ibt}-e^{iat}}{it(b-a)}$ \\
	\hline
	\makecell{\textbf{Exponencial}\\ $\sim Exp(\lambda)$} & $\displaystyle\lambda e^{-\lambda x}, x\ge 0, \lambda>0$ & $\displaystyle \frac{1}{\lambda}$ & $\displaystyle \frac{1}{\lambda^2}$ & $\displaystyle \frac{\lambda}{\lambda-it}$ \\
	\hline
	\makecell{\textbf{Normal} \\ $\sim N(\mu, \sigma^2)$} & $\displaystyle \frac{1}{\sqrt{2\pi \sigma ^2}}e^{- \frac{(x-\mu)^2}{2\sigma^2}}$ & $\mu$ & $\sigma^2$ & $\displaystyle e^{i\mu t- \frac{\sigma ^2 t^2}{2}}$ \\
	\hline
	\makecell{ \textbf{Gamma} \\ $\sim Gamma(\lambda, \tau )$} & $\displaystyle \frac{\lambda^\tau }{\Gamma (\tau )}x^{\tau -1}e^{-\lambda x}, x>0, \lambda, \tau >0$ & $\displaystyle\frac{\tau}{\lambda}$ & $\displaystyle\frac{\tau}{\lambda^2}$ & $\displaystyle \left( 1-\frac{it}{\lambda} \right)^{-\tau } $\\
	\hline
	\makecell{\textbf{Beta}\\ $\sim Beta(\alpha , \beta )$} & $\displaystyle \frac{\Gamma (\alpha +\beta)}{\Gamma (\alpha ) \Gamma (\beta )} x^{\alpha -1}(1-x)^{\beta -1}, x\in [0, 1]$ & $\displaystyle \frac{\alpha}{\alpha +\beta }$ & $\displaystyle \frac{\alpha \beta }{(\alpha + \beta )^2(\alpha +\beta +1)}$ & Sin forma sencilla\\
	\hline
	\makecell{\textbf{Weibull}\\ $\sim Weibull(\alpha , \beta )$} & $\displaystyle \frac{\alpha}{\beta } \left(\frac{x}{\beta }\right)^{\alpha -1}e^{-(x /\beta )^\alpha }, x, \alpha , \beta >0$ & $\displaystyle\beta \Gamma \left(1+\frac{1}{\alpha }\right)$ & $\displaystyle \beta ^2\left[\Gamma (1+\frac{2}{\alpha })-\Gamma^2(1+\frac{1}{\alpha })\right]$ & $\displaystyle \sum_{k\ge 0} \frac{(it)^k\beta^k}{k!}\Gamma (1+\frac{k}{\alpha })$ \\
	\hline
	\makecell{\textbf{Cauchy} \\ $\sim Cauchy(\theta , \gamma)$} & $\displaystyle \frac{1}{\pi \gamma} \frac{1}{1+(\frac{x-\theta }{\gamma})^2}, \gamma >0$ & No definida & No definida & $\displaystyle e^{\theta it-\gamma |t|}$ \\
	\hline
	$\chi^2_p$ & $\displaystyle \frac{1}{\Gamma (p /2) 2^{p /2}}x^{\frac{p}{2}-1} e^{-\frac{x}{2}}, x>0, p\in\mathbb{N}$ & $p$ & $2p$ & $\displaystyle (1-2it)^{-\frac{p}{2}}$\\
	\hline
	\makecell{\textbf{Doble expon} \\ $\sim DobExp(\mu, \gamma )$} & $\displaystyle \frac{1}{2\gamma }e^{-\frac{|x-\mu|}{\gamma }}, \gamma >0$ & $\mu$ & $2\gamma ^2$ & $\displaystyle \frac{e^{\mu it}}{1+\gamma^2t^2}$\\
	\hline
	\makecell{\textbf{Lognormal} \\ $\sim LogN(\mu, \sigma ^2)$} & $\displaystyle \frac{1}{\sigma \sqrt{2\pi} }\frac{1}{x} e^{- \frac{(\ln x -\mu)^2}{2\sigma ^2}}, x, \sigma >0$ & $\displaystyle e^{\mu+ \frac{\sigma ^2}{2}}$ & $\displaystyle e^{2(\mu+\sigma ^2)} - e^{2\mu+ \sigma ^2}$ & Sin forma sencilla\\
	\hline
  \end{tabular}
\end{center}


\newpage

\begin{multicols}{2}[\columnsep2em]

\section{Cosas útiles}
\begin{align*}
  &\sum_{0}^\infty nx^{n-1} = \frac{1}{(1-x)^2} \\
  &\Gamma (n) = (n-1)!, \quad B(a, b) = \int_0^1 t^{a-1}(1-t)^{b-1}dt = \frac{\Gamma (a)\Gamma (b)}{\Gamma (a+b)}\\
  &\Gamma (a, \tau ) +  \Gamma (b, \tau ) \sim \Gamma (a+b, \tau ) , \quad c\Gamma(a, \tau ) \sim  \Gamma (a, c\tau )\\
  &aN(\mu, \sigma ^2) + b \sim N(a\mu+b, (a\sigma )^2) \\
  &aN(\mu_1, \sigma_1^2) + bN(\mu_2, \sigma _2^2) = N(a\mu_1+b\mu_2, (a\sigma_1)^2+(b\sigma _2)^2)
\end{align*}

\section{Intro}

 \begin{proposition}
Sea $\overline{X}$ absolutamente continua con $f_{\overline{X}}(x_1,\ldots,x_n)$. Sea $g:\mathbb{R}^n\to \mathbb{R}^n \in \mathcal{C}^1$ biyectiva. Si  $g(\overline{X})=\overline{Y}$ encontramos $f_{\overline{Y}}$ de la siguiente forma:

Sea $h:=g^{-1}$:
\[
f_{\overline{Y}}(y_1, \ldots, y_n) = f_{\overline{X}}(h(y_1,\ldots,y_n))|J_h(y_1,\ldots,y_n)|
\] 
\end{proposition}

\begin{definition}[Varianza y covarianza]
  \begin{align*}
&s_x^2 = \frac{1}{N}\sum (x_i-\overline{x}_N)^2\\
&s_{xy}=\frac{1}{N}\sum(x_i-\overline{x}_N)(y_i-\overline{y}_N) \\
&r_{xy} = \frac{s_{xy}}{s_xs_y}
  \end{align*}
\end{definition}

\begin{definition}[Parámetros a estimar]
\[
X \sim F \Rightarrow \theta = \Phi(F)
\] 
\end{definition}

\begin{definition}[Estimador] $X_1, \ldots, X_n$
\[
\hat{\theta } = T(X_1, \ldots, X_n)
\] 
\end{definition}

\begin{definition}[Función de distribución empírica]
  \[
	F_n(x) = \frac{1}{n}\sum I_{(\infty, x]}(x_i)
  \] 
\end{definition}

\begin{theorem}[]$X_1, \ldots, X_n, x \in \mathbb{R}$
  \begin{enumerate}[topsep=-6pt, itemsep=0pt]
	\item $E(F_n(x))=F(x), \quad Var(F_n(x))= \frac{1}{n}F(x)(1-F(x))$
	\item $F_n(x) \to  F(x)$ casi seguro
	\item $\frac{\sqrt{n} (F_n(x)-F(x))}{\sqrt{F(x)(1-F(x))} } = \to  N(0,1)$ en distribución
	\item $\frac{\sum (x_i -\mu)}{\sigma \sqrt{n}} = \frac{\sqrt{n} (\bar{X}_n-\mu) }{\sigma } \to N(0,1)$, \ TCL
  \end{enumerate}
\end{theorem}

\begin{theorem}[Glivenko-Cantelli] $\{X_n\}$ vaiid 
  \[
	\sup_{x\in \mathbb{R}} |F_n(x)-F(x)| \to 0
  \] 
\end{theorem}

\begin{theorem}[]
Sean $x_1, \ldots, x_n$,  $S^2 =\frac{1}{n-1}\sum (x_i-\overline{x}) ^2$ la varianza muestral
\begin{enumerate}[topsep=-6pt, itemsep=0pt]
  \item $\min_a \sum (x_i-a)^2 = \sum (x_i-\overline{x})$ 
  \item $(n-1)S^2 = \sum (x_i-\overline{x})^2 = \sum x_i^2-n\overline{x}^2$
\end{enumerate}
\end{theorem}

\begin{definition}[Media muestral]
$\overline{X} = \frac{1}{n}\sum X_i$
\end{definition}

\begin{definition}[Varianza muestral]
$S^2 = \frac{1}{n-1}\sum (X_i-\overline{X})^2$
\end{definition}

\begin{theorem}[] $X$ con media  $\mu$ y varianza $\sigma ^2$  
\begin{enumerate}[topsep=-6pt, itemsep=0pt]
  \item $E(\overline{X}) = \mu$
  \item $V(\overline{X}) = \frac{\sigma ^2}{n}$ 
  \item $E(S^2)=\sigma ^2$
\end{enumerate}
\end{theorem}

\begin{proposition}
\[
N(\mu_1, \sigma_1^2) + N(\mu_2, \sigma _2^2) = N(\mu_1+\mu_2, \sigma_1^2+\sigma _2^2)
\]  
\end{proposition}

\begin{theorem}[]
\[
  \psi _{\overline{X}}(t) = (\psi_X \left( \frac{t}{n} \right) )^n
\] 
\end{theorem}

\begin{definition}[Distribución $\chi_k^2$] Sea $X_i \sim N(0,1)$
\[
\chi_k^2 = \sum X_i^2 = \Gamma\left(\frac{n}{2}, \frac{1}{2}\right)
\] 
\end{definition}

\begin{proposition} Sea $\overline{X} \sim N_p(\overline{\mu}, \Sigma)$ 
  \[
  \overline{Y} = A\overline{X} \sim N_p(A\overline{\mu}, A\Sigma A^t)
  \] 
\end{proposition}

\begin{proposition}Sea $\overline{X}\sim N_p(\overline{\mu}, \Sigma)$ 
  \[
	(\overline{X}-\overline{\mu})^t\Sigma^{-1}(\overline{X}-\overline{\mu})\sim \chi^2_{p}
  \] 
\end{proposition}

\begin{theorem}[Fisher] $X_i \sim  N(\mu, \sigma ^2)$ vaiid entonces
\[
\overline{X} \sim N\left(\mu,\frac{\sigma ^2}{n}\right), \quad \frac{(n-1)}{\sigma ^2}S^2 \sim \chi _{n-1}^2 \quad \text{ indep}
\] 
\end{theorem}

\begin{definition}[t de Student] Sean $X \sim N(0,1), Y\sim \chi _r^2$ indep
\[
T = \frac{X}{\sqrt{\frac{Y}{r}}} \sim t(r), \quad f_T(t) = \frac{\Gamma (\frac{r+1}{2})}{\sqrt{\pi r}\Gamma \left( \frac{r}{2} \right)  } \left( 1+ \frac{t^2}{r} \right)^{-\frac{r+1}{2}}
\] 
Si  $r>1 \Rightarrow E(T)=0$, si $r>2  \Rightarrow Var(T) = \frac{r}{r-1}$
\end{definition}

\begin{proposition}Sean $X_i \sim N(\mu, \sigma )$ vaiid
  \[
  \frac{\sqrt{n} (\overline{X}-\mu}{S} \sim t(n-1)
  \] 
\end{proposition}

Cosas de la F (puto palo)

\begin{definition}[Localización y escala]
\[
  X = \mu + \sigma Z, \quad f(x|\mu, \sigma )= \frac{1}{\sigma }f\left( \frac{x-\mu}{\sigma } \right) 
\]
\begin{itemize}[topsep=-6pt, itemsep=0pt]
  \item $Z \sim f(x) \iff X = \mu + \sigma Z \sim f(x|\mu, \sigma )$
  \item $X\sim f(x|\mu, \sigma ) \iff \frac{X-\mu}{\sigma } \sim  f(x)$
\end{itemize}
\end{definition}

\begin{definition}[Estimador plug-in]
 \[
   F_n(x) = \frac{1}{n} \sum I_{(-\infty, x]}(X_i), \Rightarrow \hat{\theta }_n = \Phi (F_n)
\] 
\end{definition}

\begin{definition}[Método de los momentos]
Sea $\mu_k = E(X^k)$ y que existe una biyección
\[
\mu_i = g_i(\theta_1, \ldots, \theta _k) \iff \theta _i = h_i(\mu_1, \ldots, \mu _k)
\] 
entonces $\hat{\theta }_i = h_i(m_1,\ldots, m_k)$ con $m_j = \frac{1}{n}\sum X^j$
\end{definition}

\begin{definition}[Estimador máximo verosímil]
\[
l(\theta , \overline{x})= \ln L(\theta , \overline{x}) = \ln f_{\overline{x}}(\theta ) \Rightarrow \hat{\theta } = \arg \max_{\theta } l(\theta , \overline{x})
\] 
\end{definition}

\begin{definition}[Kullback-Leiber]
  \begin{align*}
  &D_{K, L}(f \| g) = \int_S \ln\left( \frac{f(x)}{g(x)}\right)f(x)dx \ge 0\\
  &E_f(\ln(f(X)))\ge E_f(\ln(g(X)))
  \end{align*}
\end{definition}

\begin{theorem}[Bayes]
  \[
  f_{X|Y=y}(x)= \frac{P(Y=y|X=x)f_X(x)}{P(Y=y)}
  \] 

\end{theorem}

\section{Tema 3}
\begin{definition} $T_n(X_1,\ldots, X_n)$ estadístico. Llamamos \textbf{distribución de muestreo} a la distribución de $T_n$ y  \textbf{error estándar} a la desviación estándar de $T_n$.
\end{definition}
\begin{definition}[Sesgo]
$B(\hat{\theta })=E(\hat{\theta })-\theta$
\end{definition}

\begin{definition}[ECM] $ECM(\hat{\theta })=E((\hat{\theta }-\theta )^2) = (B(\hat{\theta }))^2+ Var(\hat{\theta})$
\end{definition}


\begin{theorem}[Cramer-Rao] Sea $W(X)$ un estimador insesgado para $\tau (\theta )$
  \[
	Var(W(\overline{X}))\ge \frac{\left(\frac{d \tau (\theta )}{d \theta } \right)^2}{E\left[ \left( \frac{\partial}{\partial\theta } \log f(X|\theta ) \right)^2 \right]}
  \] 
\end{theorem}

\begin{definition}[Información de Fisher]
  \begin{align*}
  I_{\overline{x}}(\theta ) = E\left[ \left(  \frac{\partial}{\partial \theta } \ln f_{\overline{x}} \right) ^2 \right]  = Var\left(\frac{\partial}{\partial \theta }\ln f_{\overline{x}}\right) =  -E\left[  \frac{\partial^2}{\partial \theta ^2} \ln f_{\overline{x}} \right]
  \end{align*}
\end{definition}

\begin{proposition} $I_{\overline{x}}(\theta ) = nI_{x_i}(\theta )$
\end{proposition}

\begin{theorem}[Alcanzar CR] $W(\overline{X})$ estimador de $\tau (\theta )$ alcanza CR  $\iff$
\[
\frac{\partial}{\partial \theta } \ln f_{\overline{x}} = a(\theta )(W(\overline{X})-\tau (\theta ))
\] 
\end{theorem}

\begin{definition}[Estadístico suficiente] $\frac{f(\overline{x}|\theta )}{q(T(\overline{x})|\theta )}$ no dep. de $\theta $
\end{definition}

\begin{theorem}[Factorización] $T(\overline{X})$ suficiente $\iff$ 
\[
f(\overline{x}; \theta ) = g(T(\overline{x});\theta )h(\overline{x})
\] 
\end{theorem}

\begin{definition}[Consistencia] La sucesión $\hat{\theta}_n = T(\overline{X})$ es consistente si 
\[
\lim P(|\hat{\theta }_n-\theta |<\varepsilon ) = 1
\] 
\end{definition}

\begin{theorem} Si $\hat{\theta }_n$ verifica
  \begin{itemize}[topsep=-6pt, itemsep=0pt]
    \item $\lim Var(\hat{\theta }_n) = 0 \ \forall \theta$
	\item $\lim B(\hat{\theta }_n,;n) = 0 \ \forall \theta $
  \end{itemize}
entonces $\hat{\theta}_n$ es consistente en media cuadrática y en probabilidad
\end{theorem}

\begin{theorem} $\hat{\theta }_n$ consistente
  \begin{enumerate}[topsep=-6pt, itemsep=0pt]
    \item $a_n \to  1, \ b_n \to  0 \Rightarrow (a_n \hat{\theta }_n + b_n$ consistente
	\item $g$ continua $\Rightarrow g(\hat{\theta}_n)$ consistente para $g(\theta)$
	\item $\hat{\delta}_n$ estimadores consistentes para $\delta$, $g(\theta , \delta)$ continua  $\Rightarrow g(\hat{\theta }_n, \hat{\delta}_n)$ consistente para $g(\theta , \delta)$
  \end{enumerate}
\end{theorem}

\begin{definition}[Normalidad asintótica]  $\hat{\theta }_n$ la presenta si $\ \exists v_n(\theta ) \to  0$ no negativos tal que
  \[
	\frac{(\hat{\theta }_n-\theta )}{\sqrt{v_n(\theta )} } \to_D N(0,1) \Rightarrow \hat{\theta }_n \sim AN(\theta , v_n(\theta ))
  \] 
\end{definition}

\begin{definition}[Eficiencia relativa asintótica] $T_n(\overline{X}), S_n(\overline{X})$ estimadores de $\theta$ asintoticamente normales varianza del orden $1 / n$
  \begin{align*}
	 &\sqrt{n}(T_n(\overline{X})-\theta )  \to _D N(0, \sigma_T^2(\theta)) \\
	 &\sqrt{n}(S_n(\overline{X})-\theta )  \to _D N(0, \sigma_S^2(\theta)) \\
	 &ARE(\theta, S_n, T_n) = \frac{\sigma ^2_T(\theta )}{\sigma ^2_S(\theta )}
  \end{align*}

\end{definition}

\begin{theorem}[Slutzky] $X_n \to _D X, Y_n \to _P a$. Entonces
  \begin{enumerate}[topsep=-6pt, itemsep=0pt]
	\item $X_n + Y_n \to _D X+a$ 
	\item $X_nY_n \to _D aX$
	\item $g(x,y)$ continua  $\Rightarrow g(X_n, Y_n) \to _D g(x,y)$
  \end{enumerate}
\end{theorem}

\begin{theorem}[Método delta] $a_n \to \infty$
  \begin{align*}
  &a_n(\hat{\theta }_n-\theta ) \to _D N(0, \sigma ^2(\theta )) \Rightarrow \\
  & \Rightarrow a_n(g(\hat{\theta }_n)-g(\theta) ) \to _D N(0, (g'(\theta ))^2 \sigma ^2(\theta ))
  \end{align*}
  Donde $\sigma ^2(\theta) = Var(X)$ (estoy 0 seguro)
\end{theorem}

\begin{theorem}[] Si se verifican 
  \begin{enumerate}[topsep=-6pt, itemsep=0pt]
    \item $\theta $ identificable
	\item $\{x:f(x|\theta )>0\}$ es el mismo $\ \forall \theta $
	\item $E_{\theta _0}( \log (\frac{f(X|\theta )}{f(X|\theta _0)}))$ existe $\ \forall \theta $ 
	\item  $\Theta$ es un abierto
	\item  $\frac{\partial f(x|\theta )}{\partial \theta }$ es continua
  \end{enumerate}
  entonces si 1, 2, y 3:
  \[
	E_{\theta _0}\left[ \log \left( \frac{L(\theta |X_n)}{L(\theta _{0}|X_n)} \right)  \right] <0, \lim P_{\theta _0}\{L(\theta _0|X_n)>L(\theta |X_n)\} = 1
  \] 
  Si además 4 y 5: $\quad \displaystyle \frac{\partial}{\partial\theta }\log L(\theta |X_n)=0$ 
\end{theorem}

\begin{theorem} Si además $\ \exists \frac{\partial^3}{\partial \theta ^3}$ acotada por $K(x)$ tal que  $E_\theta (K(X))\le k$ tenemos que si $\hat{\theta }_n \to _P \theta _0$ 
  \[
  \sqrt{n}(\hat{\theta }_n-\theta_0) \to _D N(0, \frac{1}{I_X(\theta _0)}), \quad X \sim  f(x;\theta _0)
  \] 
\end{theorem}

\begin{theorem} Los estimadores
  \[
  O_n = -\frac{\partial^2 \log L(\theta | X_n)}{\partial\theta ^2}|_{\theta =\hat{\theta }_n}, \quad E_n = I_{X_n}(\hat{\theta }_n) 
  \] 
  divididos por $n$ son estimadores consistentes de  $I_X(\theta _0)$

\end{theorem}







\section{Estimación por intervalos}

\begin{definition}[Estimador por intervalos] $[L(\overline{X}), U(\overline{X})]$
\end{definition}

\begin{definition}[Probabilidad de cobertura]
$P(\theta \in [L(X), U(X)])$
\end{definition}

\begin{definition}[Coeficiente de cobertura (confianza)] 
$\inf_{\theta }P(\theta \in [L(X), U(X)])$
\end{definition}

\begin{definition}[Intervalo de confianza] $IC_{1-\alpha }(\theta)$
\end{definition}

\begin{proposition} 
  $IC_{1-\alpha }$ de $\theta$ es $[L(X), U(X)] \Rightarrow IC_{1-\alpha}$ de $\tau (\theta )$ es $[\tau (L(X), U(X)]$ si es creciente (al revés si decreciente)
\end{proposition}

\begin{definition}[Cantidad pivotal] $Q(\overline{X}, \theta )$ es cantidad pivotal si $f_Q$ no depende de  $\theta $.
\end{definition}

\begin{proposition}
  Sea $A_\alpha : P(Q(\overline{X}, \theta )\in A_\alpha) = 1-\alpha$ entonces $C_\alpha (\overline{x}) = \{\alpha : Q(x,\theta )\in A_\alpha \}$ es conjunto de confianza
\end{proposition}

\begin{proposition}$N(\mu, \sigma )$ con $\sigma $ conocida\\
  $IC_{1-\alpha } = [\overline{x}_n - z_{\alpha /2} \frac{\sigma }{\sqrt{n} }, \overline{x}_n + z_{\alpha /2} \frac{\sigma }{\sqrt{n} }]$ \\
  $IC_{1-\alpha } = [\overline{x}_n - t_{n-1,\alpha /2} \frac{S_n }{\sqrt{n} }, \overline{x}_n + t_{n-1,\alpha /2} \frac{S_n }{\sqrt{n} }]$
\end{proposition}

\begin{definition}[Cant. pivotal asintótica] $Q_n(\overline{X}_n, \theta ) \to_D Q$ 
  \[
  Q_n = \frac{T(\overline{X}_n) - E_{\theta }^A (T(\overline{X}_n))}{\sqrt{V_\theta ^A(T(\overline{X}_n))} } \to _D N(0,1)
  \] 
\end{definition}

\begin{proposition}$[\hat{\theta }_n \mp z_{\alpha /2}\hat{se}(\hat{\theta }_n)]$ intervalo de confianza $(1-\alpha )$ asintótico para $\mu$
\end{proposition}

\begin{definition}[Intervalo EMV] $\left[\hat{\theta }_n \mp z_{\alpha /2} \sqrt{(\hat{I}_n(\theta ))^{-1}} \right]$
  \[
	Q_n^{EMV} = \frac{\hat{\theta }_n-\theta }{\sqrt{(I_n(\theta ))^{-1}} } \to _D Z \sim N(0,1)
  \] 
\end{definition}

\begin{definition}[Función score] $S_n = \frac{\partial}{\partial \theta } \log L$
  \[
	Q_n^{S} = \frac{S_n}{\sqrt{(I_n(\theta ))} } \to _D Z \sim N(0,1)
  \] 
\end{definition}

\begin{definition}[Precisión] $\Delta $ es la mitad del intervalo
\end{definition}


\section{Contrastes de hipótesis}
\begin{definition} $H_0$ hipótesis nula, $H_1$ hipótesis alternativa
\end{definition}

\begin{definition}[Contraste de hipótesis] $\begin{cases}
  H_0: F \in \mathcal{F}_0\\
  H_1: F \in \mathcal{F}_1
\end{cases}$ 
es una función de decisión $D:\mathcal{X}^n \to \{0,1\}, \overline{x} \mapsto D(x)$
\end{definition}

\begin{definition} $R$ región crítica tal que  $D(R)=1$
\begin{align*}
  \alpha = P(X\in R|H_0 \text{ cierta}) \text{ (significación)}\\
  \eta = P(X \not\in  R|H_0 \text{ falsa})
\end{align*}
\end{definition}

\begin{definition}[Potencia del test] $\beta = 1-\eta$ probabilidad de decidir de forma acertada rechazar $H_0$
\end{definition}

\begin{definition}[p-valor] ínfimo de los $\alpha $ para los cuales se rechazaría la hipótesis nula.
  Probabilidad de que, siendo $H_0$ cierta, se observe otra muestra que sea al menos tan poco favorable a la hipótesis nula como la que se ha observado.
\end{definition}

\begin{proposition}$p(\overline{x}) = \sup P(T(\overline{X})\ge T(\overline{x}))$ 
\end{proposition}

\begin{definition}[Función de potencia]
\[
\beta (\theta ) = P(X\in R) = \begin{cases}
  P(\text{Error tipo I}) & \text{ si } \theta \in \Theta_1 \\
  1-P(\text{Error tipo II}) & \text{ si } \theta \in \Theta_2 
\end{cases}
\] 
\end{definition}

\begin{definition}[Nivel] $\sup \beta (\theta )\le \alpha $
\end{definition}

\begin{theorem}[Neyman-Pearson]
\begin{align*}
  & R = \left\{ \overline{x} : \frac{L(\theta _1;\overline{x})}{L(\theta _0;\overline{x})}\ge A_\alpha  \right\}\\
  & p-\text{valor} = P\left( \frac{L(\theta _1;\overline{X})}{L(\theta _0; \overline{X})}\ge \frac{L(\theta _1;\overline{x})}{L(\theta _0;\overline{x})} \right) 
\end{align*}

\end{theorem}





\section{Modelo lineal normal}

\begin{definition}[Modelo teórico] $y_i|x_i \sim N(\beta _0+ \beta_1x_{1i}+ \cdots + \beta_{p-1i}, \sigma ^2)$, $e_i = y_i-\hat{y}_i$
\end{definition}

Formulación matricial $Y = X\beta + \epsilon$:
\[
Y = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} , \epsilon = \begin{pmatrix} \epsilon_1\\ \vdots \\ \epsilon_n \end{pmatrix} 
X = \begin{pmatrix} 1 & x_{11} & \cdots & x_{p-11} \\ \vdots & & & \vdots \\ 1 & x_{1i} & \cdots & x_{p-1i} \end{pmatrix} 
\] 
\begin{theorem}[Minimos cuadrados] $ b = (X^TX)^{-1}X^TY$
\end{theorem}

\begin{proposition} Una variable
\[
b_1 = \frac{\sum (x_i-\overline{x})(y_i-\overline{y})}{\sum (x_i-\overline{x})^2}, \quad b_0 = \overline{y} - b_1 \overline{x}
\]  
\end{proposition}

\begin{proposition} Con diferente varianza y $V$ matriz diagonal de varianza:
\[
b = (X^TV^{-1}X)^{-1}X^TV^{-1}Y
\] 
\end{proposition}

\begin{definition}[ANOVA decomposition] $SS_T = SS_R + SS_E$
  \begin{align*}
  & SS_T = \sum (y_i - \overline{y})^2, SS_R = \sum (y_i-\hat{y}_i)^2, SS_E = \sum (\hat{y}_i-\overline{y})^2 \\
  & s_E^2 = \frac{SS_E}{p-1}, \quad s_R^2 = \frac{SS_R}{n-p}, \quad F = \frac{s_E^2}{s_R^2}
  \end{align*}
\end{definition}

\begin{definition}[Coeficiente determinación]
  \[
  R^2 = 100 \frac{SS_E}{SS_T} = 100(1-\frac{SS_R}{SS_T}), R_{adj}^2 = 100(1- \frac{s_R^2}{s_R^2}) 
  \] 
\end{definition}

\begin{proposition} $b|X \sim N(\beta , \sigma^2(X^TX)^{-1})$
\end{proposition}

\begin{proposition} Una variable $Cov(b_0, b_1|x) = \frac{-\sigma ^2 \overline{x}}{\sum (x_i- \overline{x})^2}$
\begin{align*}
  b_0|x \sim  N(\beta_0, \frac{\sigma ^2}{n} \frac{\sum x_i^2}{\sum (x_i- \overline{x})^2}), \quad
  b_1|x \sim  N(\beta_1, \frac{\sigma ^2}{\sum (x_i- \overline{x})^2})
\end{align*} 
\end{proposition}

\begin{proposition} $s_R^2 |X \sim \frac{\sigma^2 }{n-p}\chi ^2_{n-p}$ 
\end{proposition}

\begin{proposition} $Var(b_j|x) = s_{b_j}^2$ 
  \begin{align*}
	(b_j \mp t_{n-p}^{\alpha / 2} s_{b_j}) \simeq  (b_j \mp 2 s_{b_j})
  \end{align*}
\end{proposition}

\begin{definition} $SS_{R0}$ suma sin las variables a retirar
\[
F = \frac{ (SS_{R0}-SS_R) / q}{ (SS_R) / (n-p)}, \quad \begin{cases}
  + F \Rightarrow \text{no retirar} \\
  -F \Rightarrow \text{retirar}
\end{cases}
\] 
\end{definition}



\end{multicols}
\end{document}
