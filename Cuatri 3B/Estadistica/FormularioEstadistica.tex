\documentclass[leqno]{article}
\usepackage{verbatim}
\usepackage{array} \usepackage{listings}
\usepackage{fancyvrb}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{eso-pic}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{graphicx}

% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}

% geometry
\usepackage{geometry}
\geometry{a4paper, margin=0.5in}

% paragraph length
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{proposition}{Proposition}
\newtheorem*{definition}{Definition}
\newtheorem*{observation}{Observation}

\newcommand{\incfig}[1]{%
\center
\def\svgwidth{0.9\columnwidth}
\import{./figures/}{#1.pdf_tex}
}
\newcommand{\incimg}[1]{%
\center
\includegraphics[width=0.9\columnwidth]{images/#1}
}
\pdfsuppresswarningpagegroup=1

\title{Formulario Estadística}
\author{Abel Doñate Muñoz}
\date{}

\begin{document}
\maketitle
\tableofcontents
\newpage

\textbf{Tabla de distribuciones discretas}
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
	\hline
	$Modelo$ &  $p(X=k)$ &  $E[X]$ &  $Var[X]$ &  $G_X(z)$ \\
	\hline
	\makecell{\textbf{Bernoulli}\\ $\sim Be(p)$ }  & $\begin{cases}  p(X=1)=p \\  p(X=0) = 1-p \end{cases}$ & $p$ &  $p(1-p)$ &  $(1-p) + pz$ \\
	\hline
	\makecell{\textbf{Binomial}\\ $\sim Bin(N, p)$} & $\displaystyle\binom{N}{k}p^i(1-p)^{N-k}$ & $Np$ &  $Np(1-p)$ &  $((1-p)+pz)^N$ \\
	\hline
	\makecell{\textbf{Uniforme} \\ $\sim U(1, N)$} &  $\displaystyle\frac{1}{N}$ & $\displaystyle\frac{N+1}{2}$ & $\displaystyle\frac{N^2-1}{12}$ & $\displaystyle\frac{1}{N} \frac{z(z^N-1)}{z-1}$ \\
	\hline
	\makecell{\textbf{Poisson} \\ $\sim Po(\lambda)$} & $\displaystyle\frac{\lambda^k}{k!}e^{-\lambda}$ & $\lambda$ & $\lambda$ & $\displaystyle e^{\lambda(z-1)}$ \\
	\hline
	\makecell{\textbf{Geométrica} \\ $\sim Geom(p)$} & $\displaystyle p(1-p)^{k-1}$ &  $\displaystyle\frac{1}{p}$ &  $\displaystyle\frac{1-p}{p^2}$ &  $\displaystyle \frac{pz}{1-(1-p)z}$ \\
	\hline
	\makecell{\textbf{Binomial negativa} \\ $\sim BinN(r, p)$} & $\begin{cases} 0 & \text{ si } k<r \\ \binom{k-1}{r-1}p^r(1-p)^{k-r} & \text{ si } k\ge r \end{cases}$ & $\displaystyle\frac{r}{p}$ & $\displaystyle r \frac{1-p}{p^2}$ & $\displaystyle \left( \frac{pz}{1-(1-p)z} \right)^r $ \\
	\hline
  \end{tabular}
\end{center}


\textbf{Tabla de distribuciones continuas}
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
	\hline
	$Modelo$ &  $f_X(x)$ &  $E[X]$ &  $Var[X]$ &  $G_X(z)$ \\
	\hline
	\makecell{\textbf{Uniforme}\\ $\sim U(a,b)$ }  & $\displaystyle\frac{1}{b-a}\mathbb{I}_{[a,b]}$ & $\displaystyle \frac{b+a}{2}$ &  $\displaystyle \frac{(b-a)^2}{12}$ &  $1 $ en $\displaystyle t=0, \frac{e^{ibt}-e^{iat}}{it(b-a)}$ \\
	\hline
	\makecell{\textbf{Exponencial}\\ $\sim Exp(\lambda)$} & $\displaystyle\lambda e^{-\lambda x}, x\ge 0, \lambda>0$ & $\displaystyle \frac{1}{\lambda}$ & $\displaystyle \frac{1}{\lambda^2}$ & $\displaystyle \frac{\lambda}{\lambda-it}$ \\
	\hline
	\makecell{\textbf{Normal} \\ $\sim N(\mu, \sigma^2)$} & $\displaystyle \frac{1}{\sqrt{2\pi \sigma ^2}}e^{- \frac{(x-\mu)^2}{2\sigma^2}}$ & $\mu$ & $\sigma^2$ & $\displaystyle e^{i\mu t- \frac{\sigma ^2 t^2}{2}}$ \\
	\hline
	\makecell{ \textbf{Gamma} \\ $\sim Gamma(\lambda, \tau )$} & $\displaystyle \frac{\lambda^\tau }{\Gamma (\tau )}x^{\tau -1}e^{-\lambda x}, x>0, \lambda, \tau >0$ & $\displaystyle\frac{\tau}{\lambda}$ & $\displaystyle\frac{\tau}{\lambda^2}$ & $\displaystyle \left( 1-\frac{it}{\lambda} \right)^{-\tau } $\\
	\hline
	\makecell{\textbf{Beta}\\ $\sim Beta(\alpha , \beta )$} & $\displaystyle \frac{\Gamma (\alpha +\beta)}{\Gamma (\alpha ) \Gamma (\beta )} x^{\alpha -1}(1-x)^{\beta -1}, x\in [0, 1]$ & $\displaystyle \frac{\alpha}{\alpha +\beta }$ & $\displaystyle \frac{\alpha \beta }{(\alpha + \beta )^2(\alpha +\beta +1)}$ & Sin forma sencilla\\
	\hline
	\makecell{\textbf{Weibull}\\ $\sim Weibull(\alpha , \beta )$} & $\displaystyle \frac{\alpha}{\beta } \left(\frac{x}{\beta }\right)^{\alpha -1}e^{-(x /\beta )^\alpha }, x, \alpha , \beta >0$ & $\displaystyle\beta \Gamma \left(1+\frac{1}{\alpha }\right)$ & $\displaystyle \beta ^2\left[\Gamma (1+\frac{2}{\alpha })-\Gamma^2(1+\frac{1}{\alpha })\right]$ & $\displaystyle \sum_{k\ge 0} \frac{(it)^k\beta^k}{k!}\Gamma (1+\frac{k}{\alpha })$ \\
	\hline
	\makecell{\textbf{Cauchy} \\ $\sim Cauchy(\theta , \gamma)$} & $\displaystyle \frac{1}{\pi \gamma} \frac{1}{1+(\frac{x-\theta }{\gamma})^2}, \gamma >0$ & No definida & No definida & $\displaystyle e^{\theta it-\gamma |t|}$ \\
	\hline
	$\chi^2_p$ & $\displaystyle \frac{1}{\Gamma (p /2) 2^{p /2}}x^{\frac{p}{2}-1} e^{-\frac{x}{2}}, x>0, p\in\mathbb{N}$ & $p$ & $2p$ & $\displaystyle (1-2it)^{-\frac{p}{2}}$\\
	\hline
	\makecell{\textbf{Doble expon} \\ $\sim DobExp(\mu, \gamma )$} & $\displaystyle \frac{1}{2\gamma }e^{-\frac{|x-\mu|}{\gamma }}, \gamma >0$ & $\mu$ & $2\gamma ^2$ & $\displaystyle \frac{e^{\mu it}}{1+\gamma^2t^2}$\\
	\hline
	\makecell{\textbf{Lognormal} \\ $\sim LogN(\mu, \sigma ^2)$} & $\displaystyle \frac{1}{\sigma \sqrt{2\pi} }\frac{1}{x} e^{- \frac{(\ln x -\mu)^2}{2\sigma ^2}}, x, \sigma >0$ & $\displaystyle e^{\mu+ \frac{\sigma ^2}{2}}$ & $\displaystyle e^{2(\mu+\sigma ^2)} - e^{2\mu+ \sigma ^2}$ & Sin forma sencilla\\
	\hline
  \end{tabular}
\end{center}


\newpage

\begin{multicols}{2}[\columnsep2em]

\section{Cosas útiles}
\begin{align*}
  &\sum_{0}^\infty nx^{n-1} = \frac{1}{(1-x)^2} \\
  &\Gamma (n) = (n-1)!, \quad B(a, b) = \int_0^1 t^{a-1}(1-t)^{b-1}dt = \frac{\Gamma (a)\Gamma (b)}{\Gamma (a+b)}\\
  &\Gamma (a, \tau ) +  \Gamma (b, \tau ) \sim \Gamma (a+b, \tau ) , \quad c\Gamma(a, \tau ) \sim  \Gamma (a, c\tau )\\
  &aN(\mu, \sigma ^2) + b \sim N(a\mu+b, (a\sigma )^2) \\
  &aN(\mu_1, \sigma_1^2) + bN(\mu_2, \sigma _2^2) = N(a\mu_1+b\mu_2, (a\sigma_1)^2+(b\sigma _2)^2)
\end{align*}

\section{Intro}

 \begin{proposition}
Sea $\overline{X}$ absolutamente continua con $f_{\overline{X}}(x_1,\ldots,x_n)$. Sea $g:\mathbb{R}^n\to \mathbb{R}^n \in \mathcal{C}^1$ biyectiva. Si  $g(\overline{X})=\overline{Y}$ encontramos $f_{\overline{Y}}$ de la siguiente forma:

Sea $h:=g^{-1}$:
\[
f_{\overline{Y}}(y_1, \ldots, y_n) = f_{\overline{X}}(h(y_1,\ldots,y_n))|J_h(y_1,\ldots,y_n)|
\] 
\end{proposition}

\begin{definition}[Varianza y covarianza]
  \begin{align*}
&s_x^2 = \frac{1}{N}\sum (x_i-\overline{x}_N)^2\\
&s_{xy}=\frac{1}{N}\sum(x_i-\overline{x}_N)(y_i-\overline{y}_N) \\
&r_{xy} = \frac{s_{xy}}{s_xs_y}
  \end{align*}
\end{definition}

\begin{definition}[Parámetros a estimar]
\[
X \sim F \Rightarrow \theta = \Phi(F)
\] 
\end{definition}

\begin{definition}[Estimador] $X_1, \ldots, X_n$
\[
\hat{\theta } = T(X_1, \ldots, X_n)
\] 
\end{definition}

\begin{definition}[Función de distribución empírica]
  \[
	F_n(x) = \frac{1}{n}\sum I_{(\infty, x]}(x_i)
  \] 
\end{definition}

\begin{theorem}[]$X_1, \ldots, X_n, x \in \mathbb{R}$
  \begin{enumerate}[topsep=-6pt, itemsep=0pt]
	\item $E(F_n(x))=F(x), \quad Var(F_n(x))= \frac{1}{n}F(x)(1-F(x))$
	\item $F_n(x) \to  F(x)$ casi seguro
	\item $\frac{\sqrt{n} (F_n(x)-F(x))}{\sqrt{F(x)(1-F(x))} } \to  N(0,1)$ en distribución
	\item $\frac{\sum (x_i -\mu)}{\sigma \sqrt{n}} \to N(0,1)$, \ TCL
  \end{enumerate}
\end{theorem}

\begin{theorem}[Glivenko-Cantelli] $\{X_n\}$ vaiid 
  \[
	\sup_{x\in \mathbb{R}} |F_n(x)-F(x)| \to 0
  \] 
\end{theorem}

\begin{theorem}[]
Sean $x_1, \ldots, x_n$,  $S^2 =\frac{1}{n-1}\sum (x_i-\overline{x}) ^2$ la varianza muestral
\begin{enumerate}[topsep=-6pt, itemsep=0pt]
  \item $\min_a \sum (x_i-a)^2 = \sum (x_i-\overline{x})$ 
  \item $(n-1)S^2 = \sum (x_i-\overline{x})^2 = \sum x_i^2-n\overline{x}^2$
\end{enumerate}
\end{theorem}

\begin{definition}[Media muestral]
$\overline{X} = \frac{1}{n}\sum X_i$
\end{definition}

\begin{definition}[Varianza muestral]
$S^2 = \frac{1}{n-1}\sum (X_i-\overline{X})^2$
\end{definition}

\begin{theorem}[] $X$ con media  $\mu$ y varianza $\sigma ^2$  
\begin{enumerate}[topsep=-6pt, itemsep=0pt]
  \item $E(\overline{X}) = \mu$
  \item $V(\overline{X}) = \frac{\sigma ^2}{n}$ 
  \item $E(S^2)=\sigma ^2$
\end{enumerate}
\end{theorem}

\begin{proposition}
\[
N(\mu_1, \sigma_1^2) + N(\mu_2, \sigma _2^2) = N(\mu_1+\mu_2, \sigma_1^2+\sigma _2^2)
\]  
\end{proposition}

\begin{theorem}[]
\[
  \psi _{\overline{X}}(t) = (\psi_X \left( \frac{t}{n} \right) )^n
\] 
\end{theorem}

\begin{definition}[Distribución $\chi_k^2$] Sea $X_i \sim N(0,1)$
\[
\chi_k^2 = \sum X_i^2 = \Gamma\left(\frac{n}{2}, \frac{1}{2}\right)
\] 
\end{definition}

\begin{proposition} Sea $\overline{X} \sim N_p(\overline{\mu}, \Sigma)$ 
  \[
  \overline{Y} = A\overline{X} \sim N_p(A\overline{\mu}, A\Sigma A^t)
  \] 
\end{proposition}

\begin{proposition}Sea $\overline{X}\sim N_p(\overline{\mu}, \Sigma)$ 
  \[
	(\overline{X}-\overline{\mu})^t\Sigma^{-1}(\overline{X}-\overline{\mu})\sim \chi^2_{p}
  \] 
\end{proposition}

\begin{theorem}[Fisher] $X_i \sim  N(\mu, \sigma ^2)$ vaiid entonces
\[
\overline{X} \sim N\left(\mu,\frac{\sigma ^2}{n}\right), \quad \frac{(n-1)}{\sigma ^2}S^2 \sim \chi _{n-1}^2 \quad \text{ indep}
\] 
\end{theorem}

\begin{definition}[t de Student] Sean $X \sim N(0,1), Y\sim \chi _r^2$ indep
\[
T = \frac{X}{\sqrt{\frac{Y}{r}}} \sim t(r), \quad f_T(t) = \frac{\Gamma (\frac{r+1}{2})}{\sqrt{\pi r}\Gamma \left( \frac{r}{2} \right)  } \left( 1+ \frac{t^2}{r} \right)^{-\frac{r+1}{2}}
\] 
Si  $r>1 \Rightarrow E(T)=0$, si $r>2  \Rightarrow Var(T) = \frac{r}{r-1}$
\end{definition}

\begin{proposition}Sean $X_i \sim N(\mu, \sigma )$ vaiid
  \[
  \frac{\sqrt{n} (\overline{X}-\mu}{S} \sim t(n-1)
  \] 
\end{proposition}

Cosas de la F (puto palo)

\begin{definition}[Localización y escala]
\[
  X = \mu + \sigma Z, \quad f(x|\mu, \sigma )= \frac{1}{\sigma }f\left( \frac{x-\mu}{\sigma } \right) 
\]
\begin{itemize}[topsep=-6pt, itemsep=0pt]
  \item $Z \sim f(x) \iff X = \mu + \sigma Z \sim f(x|\mu, \sigma )$
  \item $X\sim f(x|\mu, \sigma ) \iff \frac{X-\mu}{\sigma } \sim  f(x)$
\end{itemize}
\end{definition}

\begin{definition}[Estimador plug-in]
 \[
   F_n(x) = \frac{1}{n} \sum I_{(-\infty, x]}(X_i), \Rightarrow \hat{\theta }_n = \Phi (F_n)
\] 
\end{definition}

\begin{definition}[Método de los momentos]
Sea $\mu_k = E(X^k)$ y que existe una biyección
\[
\mu_i = g_i(\theta_1, \ldots, \theta _k) \iff \theta _i = h_i(\mu_1, \ldots, \mu _k)
\] 
entonces $\hat{\theta }_i = h_i(m_1,\ldots, m_k)$ con $m_j = \frac{1}{n}\sum X^j$
\end{definition}

\begin{definition}[Estimador máximo verosímil]
\[
l(\theta , \overline{x})= \ln L(\theta , \overline{x}) = \ln f_{\overline{x}}(\theta ) \Rightarrow \hat{\theta } = \arg \max_{\theta } l(\theta , \overline{x})
\] 
\end{definition}

\begin{definition}[Kullback-Leiber]
  \begin{align*}
  &D_{K, L}(f \| g) = \int_S \ln\left( \frac{f(x)}{g(x)}\right)f(x)dx \ge 0\\
  &E_f(\ln(f(X)))\ge E_f(\ln(g(X)))
  \end{align*}
\end{definition}

\begin{theorem}[Bayes]
  \[
  f_{X|Y=y}(x)= \frac{P(Y=y|X=x)f_X(x)}{P(Y=y)}
  \] 

\end{theorem}

\section{Tema 3}
\begin{definition} $T_n(X_1,\ldots, X_n)$ estadístico. Llamamos \textbf{distribución de muestreo} a la distribución de $T_n$ y  \textbf{error estándar} a la desviación estándar de $T_n$.
\end{definition}
\begin{definition}[Sesgo]
$B(\hat{\theta })=E(\hat{\theta })-\theta$
\end{definition}

\begin{definition}[ECM] $ECM(\hat{\theta })=E((\hat{\theta }-\theta )^2) = (B(\hat{\theta }))^2+ Var(\hat{\theta})$
\end{definition}


\begin{theorem}[Cramer-Rao] Sea $W(X)$ un estimador insesgado para $\tau (\theta )$
  \[
	Var(W(\overline{X}))\ge \frac{\left(\frac{d \tau (\theta )}{d \theta } \right)^2}{E\left[ \left( \frac{\partial}{\partial\theta } \log f(X|\theta ) \right)^2 \right]}
  \] 
\end{theorem}

\begin{definition}[Información de Fisher]
  \begin{align*}
  I_{\overline{x}}(\theta ) = E\left[ \left(  \frac{\partial}{\partial \theta } \ln f_{\overline{x}} \right) ^2 \right]  = Var\left(\frac{\partial}{\partial \theta }\ln f_{\overline{x}}\right) =  -E\left[  \frac{\partial^2}{\partial \theta ^2} \ln f_{\overline{x}} \right]
  \end{align*}
\end{definition}

\begin{proposition} $I_{\overline{x}}(\theta ) = nI_{x_i}(\theta )$
\end{proposition}

\begin{theorem}[Alcanzar CR] $W(\overline{X})$ estimador de $\tau (\theta )$ alcanza CR  $\iff$
\[
\frac{\partial}{\partial \theta } \ln f_{\overline{x}} = a(\theta )(W(\overline{X})-\tau (\theta ))
\] 
\end{theorem}

\begin{definition}[Estadístico suficiente] $\frac{f(\overline{x}|\theta )}{q(T(\overline{x})|\theta )}$ no dep. de $\theta $
\end{definition}

\begin{theorem}[Factorización] $T(\overline{X})$ suficiente $\iff$ 
\[
f(\overline{x}; \theta ) = g(T(\overline{x});\theta )h(\overline{x})
\] 
\end{theorem}

\end{multicols}


\end{document}
